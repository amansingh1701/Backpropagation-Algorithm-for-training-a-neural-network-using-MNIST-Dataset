{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Backpropagation-Algorithm-for-training-a-neural-network-using-MNIST-Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyOfL9FKDoG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlcq1HAs7meI",
        "colab_type": "text"
      },
      "source": [
        "##Problem 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USiZZJaUFQcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(filepath):\n",
        "  df = pd.read_csv(filepath)\n",
        "  df.head()\n",
        "  Y = df['5'].to_numpy()\n",
        "  del df['5']\n",
        "  X=df.to_numpy()\n",
        "  return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVXIEJCvD0dJ",
        "colab_type": "code",
        "outputId": "f8118b3b-b1e8-4667-e5a8-c6f41da1940d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X, y = load_data(\"mnist_train.csv\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\\\n",
        "                X, y, test_size=0.3, random_state=42)\n",
        "#One hot encoding of training labels \n",
        "Labels=pd.get_dummies(y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b1ff8d595d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mnist_train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m                \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#One hot encoding of training labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mLabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zyoHKGD7tvJ",
        "colab_type": "text"
      },
      "source": [
        "## Problem 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aYMsXGyDwqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "\n",
        "\n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            x=self.softmax(z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            prev_term=self.delta_mll(y,self.y_pred)  \n",
        "            # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "            self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            x=self.softmax(z) \n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjgzI7Ss7dbD",
        "colab_type": "code",
        "outputId": "8d0db64f-143b-438a-9d0a-fc60e2d274b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "source": [
        "n=Perceptron(X_train,Labels)\n",
        "n.connect(X_train,Labels)\n",
        "n.train(batches=1000,lr=0.2,epoch=30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.001857108821217213\n",
            "1 0.00203081834362096\n",
            "2 0.001983248740445829\n",
            "3 0.001911112049881923\n",
            "4 0.0015657577574415276\n",
            "5 0.0014626236958980863\n",
            "6 0.0014015530129357233\n",
            "7 0.0013002117615802053\n",
            "8 0.0012805168256604547\n",
            "9 0.00123085638198447\n",
            "10 0.0009888418750220603\n",
            "11 0.0007506010327488155\n",
            "12 0.0006367100751720225\n",
            "13 0.0004946454962135487\n",
            "14 0.00040245232768978473\n",
            "15 0.0003256678019366484\n",
            "16 0.00025565068419024456\n",
            "17 0.00023729864844571137\n",
            "18 0.00023655292663566434\n",
            "19 0.0002306469351798041\n",
            "20 0.0002186126137421495\n",
            "21 0.0001970971133279053\n",
            "22 0.00017316565826273474\n",
            "23 0.00015351959031843562\n",
            "24 0.00014088878989669033\n",
            "25 0.0001357377791760879\n",
            "26 0.00013394270845773983\n",
            "27 0.00013077915932523028\n",
            "28 0.0001255291148279883\n",
            "29 0.00011920083414108736\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhDzZkLr-Bg0",
        "colab_type": "code",
        "outputId": "e854b142-0d0d-45f7-aa67-1d7b62ff22de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([313, 333, 303, 326, 320, 285, 290, 320, 218, 292]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2jK6jKC-Il3",
        "colab_type": "code",
        "outputId": "2b6be227-fae8-4b8c-b6ea-c42d4872cc13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 88.53333333333333 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klZhm_WFDeV-",
        "colab_type": "text"
      },
      "source": [
        "## Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYE-TondDhAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"\n",
        "    size: Number of nodes in the hidden layer \n",
        "    activation: name of activation function for the layer\n",
        "    \"\"\"\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class SingleLayerNeuralNetwork():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                \n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "\n",
        "        return np.argmax(x,axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wffR59W-D9zR",
        "colab_type": "code",
        "outputId": "79f24381-8765-4084-88cf-f24333c6ef34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        }
      },
      "source": [
        "n=SingleLayerNeuralNetwork(X_train,Labels)\n",
        "l1=Layer(100)\n",
        "n.connect(X_train,l1)\n",
        "n.connect(l1,Labels)\n",
        "n.train(batches=1000,lr=0.1,epoch=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.00029056420301964145\n",
            "1 0.0003601394686263144\n",
            "2 0.00037235191222934343\n",
            "3 0.0002719241319754682\n",
            "4 0.00017712082293322722\n",
            "5 0.00012931407547602707\n",
            "6 9.972558005819009e-05\n",
            "7 7.792272646956795e-05\n",
            "8 6.0971673982260514e-05\n",
            "9 4.7058994369173836e-05\n",
            "10 3.63921188920947e-05\n",
            "11 2.877901748733128e-05\n",
            "12 2.3264229798474398e-05\n",
            "13 1.9248234461062408e-05\n",
            "14 1.6375601221407347e-05\n",
            "15 1.4191274848050528e-05\n",
            "16 1.240503603414625e-05\n",
            "17 1.0913430872799652e-05\n",
            "18 9.670484844822741e-06\n",
            "19 8.647260844794485e-06\n",
            "20 7.812212970639626e-06\n",
            "21 7.131781719368547e-06\n",
            "22 6.576367753362659e-06\n",
            "23 6.121840990406356e-06\n",
            "24 5.748508951688343e-06\n",
            "25 5.4400668381939155e-06\n",
            "26 5.183121194004934e-06\n",
            "27 4.966574890976243e-06\n",
            "28 4.78094217056842e-06\n",
            "29 4.618106913072862e-06\n",
            "30 4.471454396249423e-06\n",
            "31 4.335955754688563e-06\n",
            "32 4.208041655364974e-06\n",
            "33 4.085344188455288e-06\n",
            "34 3.9664066864910425e-06\n",
            "35 3.850417319212721e-06\n",
            "36 3.736991316884114e-06\n",
            "37 3.626008914179686e-06\n",
            "38 3.5175042344275857e-06\n",
            "39 3.411593528527793e-06\n",
            "40 3.3084300621152036e-06\n",
            "41 3.208175668794762e-06\n",
            "42 3.110982599335782e-06\n",
            "43 3.016982019373743e-06\n",
            "44 2.9262770620217178e-06\n",
            "45 2.838939122350808e-06\n",
            "46 2.7550065042803126e-06\n",
            "47 2.6744848242535106e-06\n",
            "48 2.597348802086202e-06\n",
            "49 2.5235452072868115e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp0shsvSEWvN",
        "colab_type": "code",
        "outputId": "3fbd72b8-6e6c-494c-9b3f-f84c4c83c3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([303, 337, 293, 322, 318, 281, 288, 329, 233, 296]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knkPcfW8EYx5",
        "colab_type": "code",
        "outputId": "43576198-5426-438e-82a7-39133060f43e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 91.9 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcxS5MRXF1km",
        "colab_type": "text"
      },
      "source": [
        "## Problem 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V_nmBd8mF4d5",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"\n",
        "    size: Number of nodes in the hidden layer \n",
        "    activation: name of activation function for the layer\n",
        "    \"\"\"\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class DoubleLayerNeuralNetwork():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                \n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f2508719-3bbc-4b30-b357-500d491748bc",
        "id": "xJY6j6AiGCI3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        }
      },
      "source": [
        "n=DoubleLayerNeuralNetwork(X_train,Labels)\n",
        "l1=Layer(100)\n",
        "l2=Layer(100)\n",
        "n.connect(X_train,l1)\n",
        "n.connect(l1,l2)\n",
        "n.connect(l2,Labels)\n",
        "n.train(batches=1000,lr=0.1,epoch=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.00011871962483125364\n",
            "1 8.708387523884889e-05\n",
            "2 8.358341417501189e-05\n",
            "3 5.186942573736507e-05\n",
            "4 4.9391222551848324e-05\n",
            "5 5.586932424179373e-05\n",
            "6 5.955288275654046e-05\n",
            "7 6.0500651981969423e-05\n",
            "8 6.061181458985128e-05\n",
            "9 5.927468844434443e-05\n",
            "10 5.755189874936134e-05\n",
            "11 5.5551478122957354e-05\n",
            "12 5.2304961904705274e-05\n",
            "13 4.902708659587294e-05\n",
            "14 4.6473122141189856e-05\n",
            "15 4.43762292457179e-05\n",
            "16 4.227849611359948e-05\n",
            "17 3.9643107677816485e-05\n",
            "18 3.6329825497920363e-05\n",
            "19 3.2735028330788125e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRAWf6C5GJgN",
        "colab_type": "code",
        "outputId": "fb461e7c-0f3b-4936-e5a7-70dfdbef438d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([309, 326, 304, 314, 324, 242, 306, 327, 259, 289]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmpGuKUbGMJ1",
        "colab_type": "code",
        "outputId": "fe118daf-c5ce-4089-ccef-b55ea22cf623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 90.13333333333334 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCuNTnWEGOub",
        "colab_type": "text"
      },
      "source": [
        "## Problem 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I-KrbHw9GlsV",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"\n",
        "    size: Number of nodes in the hidden layer \n",
        "    activation: name of activation function for the layer\n",
        "    \"\"\"\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class NeuralNetworkActivations():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        elif name=='relu':\n",
        "            if derivative==False:\n",
        "                return np.maximum(0.0,z)\n",
        "            else:\n",
        "              z[z<=0] = 0.0\n",
        "              z[z>0] = 1.0\n",
        "              return z\n",
        "        elif name=='tanh':\n",
        "          if derivative==False:\n",
        "                return np.tanh(z)\n",
        "          else:\n",
        "                return 1.0 - (np.tanh(z)) ** 2\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "875eccac-039d-4eec-9519-642530a2a821",
        "id": "JjlQBHZCHFWA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        }
      },
      "source": [
        "n=NeuralNetworkActivations(X_train,Labels)\n",
        "l1=Layer(100,'sigmoid')\n",
        "l2=Layer(50, 'tanh')\n",
        "n.connect(X_train,l1)\n",
        "n.connect(l1,l2)\n",
        "n.connect(l2,Labels)\n",
        "n.train(batches=1000,lr=0.1,epoch=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0004307954911560534\n",
            "1 0.0004992736785950375\n",
            "2 0.0004980082297130047\n",
            "3 5.52402932903838e-05\n",
            "4 0.0004078686565631133\n",
            "5 2.380937879654742e-05\n",
            "6 0.00026332028500567536\n",
            "7 0.0009158860315001468\n",
            "8 0.00035284785794144065\n",
            "9 3.9898984935763906e-05\n",
            "10 9.610473455924203e-05\n",
            "11 0.0007457865409847738\n",
            "12 0.00013809455580304268\n",
            "13 1.2914656051145556e-06\n",
            "14 0.00024159535608584302\n",
            "15 6.195009218412286e-07\n",
            "16 1.484582282804448e-05\n",
            "17 6.286871548289794e-05\n",
            "18 2.7459368567110116e-05\n",
            "19 1.20837080719512e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZfek1XaL2PR",
        "colab_type": "code",
        "outputId": "4e7aef04-ceb4-4c07-ecc8-764c380078c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([302, 321, 322, 334, 309, 286, 284, 328, 254, 260]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd2UmE1jNEUV",
        "colab_type": "code",
        "outputId": "2b7f02f4-28df-4dd8-983b-f1498e6cfe6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 92.16666666666667 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFVOwq1RQLVo",
        "colab_type": "text"
      },
      "source": [
        "## Problem 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IyyRnpPaQRSS",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"\n",
        "    size: Number of nodes in the hidden layer \n",
        "    activation: name of activation function for the layer\n",
        "    \"\"\"\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class NeuralNetworkMomentum():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        self.delta_weights=[]\n",
        "        self.delta_bias=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.delta_weights.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.delta_bias.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        elif name=='relu':\n",
        "            if derivative==False:\n",
        "                return np.maximum(0.0,z)\n",
        "            else:\n",
        "              z[z<=0] = 0.0\n",
        "              z[z>0] = 1.0\n",
        "              return z\n",
        "        elif name=='tanh':\n",
        "          if derivative==False:\n",
        "                return np.tanh(z)\n",
        "          else:\n",
        "                return 1.0 - (np.tanh(z)) ** 2\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr,beta=0.9,momentum=False):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            if momentum:\n",
        "                self.delta_weights[i]=beta*self.delta_weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                self.delta_bias[i]=beta*self.delta_bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                self.weights[i]=self.weights[i]+self.delta_weights[i]\n",
        "                self.bias[i]=self.bias[i]+self.delta_bias[i]\n",
        "            else:\n",
        "                self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "    \n",
        "    def train(self,batches,beta,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr,beta,momentum=True)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2fc9e693-b156-45ee-b691-c2a8d495fdd7",
        "id": "5KieS6HyQuwx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        }
      },
      "source": [
        "n=NeuralNetworkMomentum(X_train,Labels)\n",
        "l1=Layer(100,'sigmoid')\n",
        "l2=Layer(50, 'tanh')\n",
        "n.connect(X_train,l1)\n",
        "n.connect(l1,l2)\n",
        "n.connect(l2,Labels)\n",
        "n.train(batches=500,beta=0.5,lr=0.1,epoch=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0010769964662069851\n",
            "1 0.000596783260274046\n",
            "2 0.00027865406514536646\n",
            "3 0.00035160909852037395\n",
            "4 9.49066291218231e-05\n",
            "5 5.76292319899219e-05\n",
            "6 5.3538093264228203e-05\n",
            "7 0.00045998380368300216\n",
            "8 1.4781374920552931e-05\n",
            "9 4.97034263015322e-05\n",
            "10 7.997169898033357e-06\n",
            "11 6.409189445722453e-06\n",
            "12 2.1931864988349394e-05\n",
            "13 1.98468533710637e-05\n",
            "14 1.4915461260387636e-05\n",
            "15 1.0360349548576659e-05\n",
            "16 8.081454822206198e-06\n",
            "17 7.3650074049607335e-06\n",
            "18 6.79707410595793e-06\n",
            "19 6.223378456637413e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6I2UDHGQy7c",
        "colab_type": "code",
        "outputId": "a32fcde1-17ce-4c63-903f-bb5c967cf59a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([302, 331, 309, 313, 301, 267, 290, 334, 254, 299]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apHXOBwRQ60d",
        "colab_type": "code",
        "outputId": "7fbc438f-1150-4384-fb54-43a55e4c4b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 93.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LHz5e1OKqin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}